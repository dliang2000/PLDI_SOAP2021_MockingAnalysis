\section{Specific application: Finding Focal Methods}
\label{sec:focal}

In Section~\ref{sec:applications} we surveyed several applications of mock analysis.
One useful application of mock detection is to
improve the detection of focal methods in test cases, or as Ghafari et
al~\cite{ghafari15:_autom} call them, focal methods under test
(F-MUTs). In this section, we delve into one specific application of mock analysis, 
discussing some approaches to finding
focal methods, and describing how our analysis can facilitate this search.

Ghafari's approach is the one that is most amenable to our work.
Their approach is applicable to tests for classes that implement
stateful objects, and assume that each test has at least one focal
method, seen as a call to a mutator method for the class under test.
They declare the last such call to be the focal method. However, their approach can
only work on tests of mutator methods.  Purely-functional methods, for
instance, are beyond the scope of their approach.

Another approach is name-based, as seen in the methods2test
dataset~\cite{tufano2020unit}.  This approach uses two heuristics: 1)
the name of the test matches the name of the focal method; or, 2) the
test calls a unique method in the focal class. Rompaey and
Demeyer~\cite{rompaey09:_estab_traceab_links_unit_test} explore the
name-based approach along with simply using the last method called---not necessarily a mutator---as the class containing the focal method.  Romaey and Demeyer also
explore other heuristics, as we discuss in Section~\ref{sec:related}. Ghafari et al point out
that these heuristics have significant limitations, e.g. when a
test case has sub-scenarios.

\paragraph{Precision and Recall}
We further compare the static analysis and name-based approaches to detecting focal methods in terms of their precision and recall. Table~\ref{tab:focal-method-algorithm-comparison} presents an approximation to recall---the percentage of test cases reported as having focal methods by each of the approaches. This approximates recall under the assumption that every test case has exactly one focal method. (In practice, a test case may have more than one focal method; we believe that a reasonable test should have at least one focal method.) The four projects presented under Ghafari's algorithm are from their paper~\cite{ghafari15:_autom}, whereas we hand-picked four benchmarks from methods2test's dataset to match (as far as possible) those in Ghafari's work.

The data suggests that methods2test's name-based approach of focal method detection has low recall. This is not surprising---their heuristics have quite strict constraints, i.e. limited applicability. We reviewed some of their mock analysis results, and found that none of their reported focal method invocations were mocks, consistent with high precision.

On the other hand, Ghafari kindly shared with us recent focal method detection results. We found that there are a few types of unit test cases where they have to manually report focal methods. The first type is where their tool automatically reports a focal method that is actually a mock invocation from the developer's standpoint (an application for \textsc{MockDetector}). On the other hand, their algorithm would return no focal method for test cases come with no assertion statements. Overall, Ghafari's algorithm requires more manual work to increase precision, but the percentage of test cases where they report focal methods (a proxy to recall) is acceptable. 

In short, methods2test's approach finds focal methods with high precision but low recall, whereas Ghafari's algorithm has an acceptable recall but low precision. Our mock analysis can help increase precision by ruling out calls that are definitely not to focal methods.

\paragraph{Example: mock objects and focal methods} We continue discussing how we rule out calls by revisiting the unit test case presented in Listing~\ref{lis:mockCall}. Figure~\ref{fig:mockExampleEvaluation} highlights the process for finding the mock object \texttt{session} and consequently the mock invocation \texttt{getRequest()} in the unit test. Both Soot and Doop implementations report this mock invocation 
from the test case. Thus, with the assistance of \textsc{MockDetector}, \texttt{getRequest()} can be removed from consideration as a focal method. We judge \texttt{getToolchainsForType()} to be the focal method since we assume that every test has at least one focal method, and this is the only method invocation remaining after the elimination process. In addition, for the return array \texttt{basics}, its \texttt{length} attribute indeed gets checked in the assertion statement on Line~\ref{line:assert}, which means this test case indeed tests the behaviour of \texttt{toolchainManager.getToolchainsForType()}. 

For this unit test, since Ghafari's algorithm does not consider accessors (``inspector methods'' in their paper) as focal methods, they will presumably report no focal method for this unit test case. Ghafari's algorithm drops recall here.
Incidentally, since Ghafari's heuristic requires the unit test case to have at least one assertion statement, the algorithm will presumably fail to return any focal methods when analyzing unit test cases without assertions.

As for the heuristic for methods2test, method \texttt{getTool\\ChainsForType()} does not appear to match under the name-based heuristic from methods2test. The other heuristic finds calls to the focal class, so methods2test should identify the call to \texttt{getToolChainsForType()} with that heuristic. Our mock analysis can help with that heuristic by removing mocks from the set of eligible focal classes.

\begin{table*}
	\centering
	\caption{Comparison of \% of test cases with reported focal methods by the two automated focal method detection algorithms.}
	\vspace*{.5em}
	\resizebox{\textwidth}{!}{\begin{tabular}{lrrrrrlrrrr} \toprule
		\multicolumn{5}{c}{Ghafari's algorithm} & & \multicolumn{5}{c}{methods2test}\\
		\cmidrule{1-5} \cmidrule{7-11}
		\thead{Benchmark} & \thead{Source Code \\ KLoC} & \thead{Reported \\ Focal Methods} & \thead{Test Cases} & \thead{\% of test cases \\ with focal \\ methods detected} &  & \thead{Benchmark} & \thead{Source Code \\ KLoC} & \thead{Reported \\ Focal Methods} & \thead{Test Cases} & \thead{\% of test cases \\ with focal \\ methods detected} \\ 
		\cmidrule{1-5} \cmidrule{7-11}
		
		commons-email-1.3.3 & 8.78 & 90  & 130 &  69\%  &  &    goja-0.1.14/goja-core  & 11.52 & 27  & 80 &  34\% \\
		PureMVC-1.0.8 & 19.46 & 34  & 43 &  79\%  &  &  mock-socket-0.9.0    & 1.09  & 4  & 34 &  12\%      \\
		XStream-1.4.4 & 54.93 & 513  & 968 &  53\%   &  &  project-sunbird-4.3.0/sunbird-lms-service   & 45.36 & 310  & 984 &  31\%   \\
		JGAP-3.4.4  & 73.96 & 1015  & 1390 &  73\%  &  &   optiq-0.8/core    & 93.94  & 26  & 1346 &  2\%   \\
		\bottomrule
		Geometric Mean &   &  &  &  68\% &   &  &  &  &  &  12\% 
	\end{tabular}}
	\label{tab:focal-method-algorithm-comparison}
\end{table*}


\begin{figure}[h]
	\begin{lstlisting}[
	basicstyle=\ttfamily\scriptsize,language = Java, framesep=4.5mm, framexleftmargin=1.0mm, captionpos=b, escapechar={|}, mathescape=true, morekeywords={@Test}]
  @Test public void testMisconfiguredToolchain() throws Exception {
    //                mock:|\cmark|    mockAPI:|\cmark|
    MavenSession |\colorbox{olive}{session}| = |\colorbox{teal}{mock}| ( MavenSession.class );
    MavenExecutionRequest req =
        new DefaultMavenExecutionRequest();
    //     mock invocation:|\cmark| $\Rightarrow$ focal method:|\xmark|
    when( session.|\colorbox{red}{getRequest()}| ).thenReturn( req ); |\label{line:Fmock}|

    ToolchainPrivate[] basics =
      //                      focal method:|\cmark|
      toolchainManager.|\colorbox{gray}{getToolchainsForType}|("basic", session); |\label{line:Freal}|

    assertEquals( 0, basics.length );|\label{line:assert}|
  }
  \end{lstlisting}

  \caption{Example: removing mock invocation from focal method consideration.}
  \label{fig:mockExampleEvaluation}
\end{figure}

\paragraph{Internal and External Mock Objects} 
We observe that two types of objects may be mocked in test cases. We will call them internal objects (defined within the project) and external objects (defined elsewhere). Developers ought to be sure that they have tests for internally-mocked object behaviours where these objects are not mocked. For example, the behaviour of \texttt{session.getRequest()} in Figure~\ref{fig:mockExampleEvaluation} should be tested elsewhere on a real \texttt{MavenSession} object. An IDE or CI tool could easily point this out to developers, once \textsc{MockDetector} has correctly identified \texttt{session} as a mock.
