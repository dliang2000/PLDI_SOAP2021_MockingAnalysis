\section{A Survey of Applications}
\label{sec:applications}

Our work supports test-to-code traceability. 
We sketch applications of our work to this important problem,
inspired by Ghafari et al~\cite{ghafari15:_autom}.

\paragraph{Test case comprehension} xUnit tests are snippets of arbitrary
code. In unpublished work, we found that most tests are
straight-line code (conditionals and loops are rare). 
However, this code by necessity interacts with
the system under test, in potentially complicated ways. Hence,
understanding what a test case is doing can be difficult. Ghafari et
al described experiments where developers are asked to identify the
focal method, and finds that this is surprisingly difficult;
furthermore, Daka et al~\cite{daka15:_model_readab_improv_unit_tests}
state that reasoning about tests is difficult and propose a model that
enables the automatic generation of tests which are especially
readable.

Our mock analysis helps segregate tests into parts that are
mock-related and parts that are not mock-related. This is directly applicable
to identifying focal methods, as we'll discuss in Section~\ref{sec:focal}.
Also, it is fairly
obvious to a human reader that the part of a test that is calling mock
library methods such as \texttt{thenReturns()} is setting up mock
expectations, but we've seen cases where this is less
obvious. In particular, when recording behaviours with EasyMock,
the developer simply calls methods on the mock object, e.g. 
\texttt{mock.documentAdded("document")}.

Knowing what is a mock can help developers debugging test case
failures: 1) when looking at
mock calls, they know that these calls are not
directly causing the test failures; but also, 2) if the mock calls
now return incorrect values (perhaps due to program evolution),
then it is likely OK to update the recorded values. 
Similar considerations
apply for automatic debugging and code repair.

\paragraph{Code recommendation}
To amplify the previous point, a key application of test-to-code
traceability is that when the code is updated, related tests may also
need to be updated. Integrated Development Environments can find
all tests referring to a particular fragment of code. Mock analysis
augments the information available to the developer (or maintainer) by
giving them additional information about whether they are updating
tests that depend on the changed code as a mock or whether the tests
are testing the changed code.

\paragraph{Automated refactoring/code generation}
We originally formulated the mock analysis problem as a side problem which needed
to be solved in the service of a deeper problem, which is still
ongoing: automatically generating certain useful tests. In our context,
we needed to know which call was to the focal method of a test---we
wanted to create new tests of focal methods,
disregarding mocks.

\paragraph{Mutation testing}
Although code coverage metrics are well established, they are most useful in highlighting what is not covered in testing, and not strongly correlated with test suite effectiveness~\cite{inozemtseva14:_cover_not_stron_correl_test_suite_effec}. Mutation testing has been gaining popularity in practice~\cite{petrovic55:_pract_mutat_testin_scale,beller2021use}.
% The idea is to generate mutants---altered version of a program which presumably have incorrect behaviour. 
Developers kill mutants (and improv test suites~\cite{Petrovic:mutation_testing}) by writing tests to distinguish between the original program and the mutant. 
%Mutants can thus incite developers to improve their test suites, and Petrovi\'c et al~\cite{Petrovic:mutation_testing} provide evidence that mutation testing does so.

Mock analysis can help developers use mutation testing as follows. Given a mutant to kill, the developer can also be told about existing test cases that touch the code changed in the mutant. Of those cases, modifying the ones that use mocks can be easier than changing those that don't, since it is easy to manipulate return values coming back from mocks.

% meh, nah
%% \paragraph{Internal mocks}
%% Consider again the \texttt{session} mock from Listing~\ref{lis:mockCall}. This test case mocks \texttt{session.getRequest()}.
%%  internal (defined within the project) and external (defined elsewhere). Developers ought to be sure that they have tests for internally-mocked object behaviours where these objects are not mocked. For example, the behaviour of \texttt{session.getRequest()} in Figure~\ref{fig:mockExampleEvaluation} should be tested elsewhere on a real \texttt{MavenSession} object. An IDE or CI tool could easily point this out to developers, once \textsc{MockDetector} has correctly identified \texttt{session} as a mock.


\paragraph{Extracting API usage examples}
Finally, mock objects can serve as even more concise API usage
examples for the objects being mocked (compared to the test as a whole);
that information could be extracted from the calls to
the mock library. After all, mocks record invocations to methods the objects
being mocked, and the expected behaviours of those objects in response to
these invocations.

In summary, our applications show that the treatment of mocks and non-mocks
when editing test cases (manually or automatically) should be different.
Furthermore, especially for automatic treatments of test cases, a static
analysis determining which invocations are mock invocations is key to effective
treatment.

%* code recommendation for editing test cases
%* automated debugging and repair that rely on failing tests to id repair subjects
%* automated refactoring
%* extracting API usage examples [Ghafari ICPC 2014]
