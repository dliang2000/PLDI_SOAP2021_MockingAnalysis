\section{A Survey of Applications}
\label{sec:applications}

Our work supports test-to-code traceability. 
We now sketch several applications of our work to this important problem,
inspired by Ghafari et al~\cite{ghafari15:_autom} but specializing to
our analysis.

\paragraph{Test case comprehension} xUnit tests are snippets of arbitrary
code. In unpublished research, we have established that most tests are
simple straight-line code (although there are still nontrivial incidences of
conditionals and loops). However, this code by necessity interacts with
the system under test in potentially complicated ways. Hence,
understanding what a test case is doing can be difficult. Ghafari et
al described experiments where developers are asked to identify the
focal method, and finds that this is surprisingly difficult;
furthermore, Daka et al~\cite{daka15:_model_readab_improv_unit_tests}
state that reasoning about tests is difficult and propose a model that
enables the automatic generation of tests which are especially
readable.

Our mock analysis helps segregate tests into parts that are
mock-related and parts that are not mock-related. This is directly applicable
to identifying focal methods, as we'll discuss in Section~\ref{sec:focal}.
Also, it is fairly
obvious to a human reader that the part of a test that is calling mock
library methods such as \texttt{thenReturns()} is setting up mock
expectations, but we've seen cases where this is less
obvious. In particular, when recording behaviours with EasyMock,
the developer simply calls methods on the mock object, e.g. 
\texttt{mock.documentAdded("document");}.

Knowing what is a mock can help developers debugging test case
failures get into the right mindset, in two ways: 1) when looking at
mock calls, they can conclude that these mock calls are not
directly causing the test failures; but also, 2) if the mock calls
are now returning incorrect values (perhaps due to program evolution),
then it is likely appropriate to update the recorded values. Similar considerations
apply for automatic debugging and code repair.

\paragraph{Code recommendation}
To amplify the previous point, a key application of test-to-code
traceability is that when the code is updated, related tests may also
need to be updated. Integrated Development Environments can find
all tests referring to a particular fragment of code. Mock analysis
can augment the information available to the developer (or maintainer) by
giving them additional information about whether they are updating
tests that depend on the changed code as a mock or whether the tests
are in fact testing the changed code itself.

\paragraph{Automated refactoring/code generation}
We originally formulated the mock analysis problem as a side problem which needed
to be solved in the service of a deeper problem, which is still
ongoing work: automatically generating certain useful tests. In our context,
we needed to know which call was to the focal method of a test---we
wanted to create additional tests based on those focal methods, but
not based on mock objects.

\paragraph{Mutation testing}
Code coverage metrics are well established in the software industry. These coverage metrics alone, however, may be insuffient to prove test quality since executing lines does not provide any insight about proper testing and use-case completeness.

Mutation testing is a promising technique often used to identify defects in programs. Each mutant is an altered version of the program with some changes in the code and aims to cause some tests fail. Defects can be identified from surviving mutants (meaning all test cases passed under the mutant). Petrovic et al~\cite{Petrovic:mutation_testing} provide evidence that surviving mutants are effective to give actionable test goals.

Identifying mock objects and mock calls in test suite could be helpful to better understand test cases containing mocks to kill the surviving mutants. It would be easy if you could alter some attributes to the mock objects that kills mutants.  Killing mutants by modifying the mock part of the tests and by modifying the non-mock part of the tests has different affordances, and we believe it worth some investigation for each approach.

\paragraph{Extracting API usage examples}
Finally, mock objects can serve as even more concise API usage
examples for the objects being mocked (compared to the test as a whole);
that information could be extracted from the calls to
the mock library. After all, mocks record invocations to methods the objects
being mocked, and the expected behaviours of those objects in response to
these invocations.

Our applications show that the treatment of mocks and non-mocks
when editing test cases (manually or automatically) should be different.
Furthermore, especially for automatic treatments of test cases, a static
analysis determining which invocations are mock invocations is key to effective
treatment.

%* code recommendation for editing test cases
%* automated debugging and repair that rely on failing tests to id repair subjects
%* automated refactoring
%* extracting API usage examples [Ghafari ICPC 2014]
