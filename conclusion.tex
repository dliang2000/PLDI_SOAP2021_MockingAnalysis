\section{Conclusion}
\label{sec:conclusion}

Our thesis is that mock objects are an important technique that developers use when creating test suites. However, common mock object libraries use reflection and cause developers to write tests that appear to have different behaviours than they actually do---method invocations on mocks look like normal method calls, but instead record behaviour. Because of the prevalence of mocks, tools that work with tests (including static analyses, IDEs, and automatic repair tools) need to correctly handle mock objects. 

We have described our \textsc{MockDetector} static analysis, which we intend to use for further static analyses of test cases. We have implemented \textsc{MockDetector} imperatively in Soot and declaratively in Doop, and characterized its performance and behaviour on a set of 8 open-source benchmarks. Our results show that mocks play an important role in achieving test coverage for some real-world benchmarks. We believe that mock analysis is a useful prerequisite for test case analysis and development, enabling numerous subsequent analyses.

%% Having described our imperative and declarative approaches to implementing mock analysis, we now comment on the strengths and weaknesses of these two approaches. We hope that our discussion will help future designers of source code analyses and frameworks.

%% \paragraph{Subsequent use of results} Doop is a standalone tool. It depends on other tools to provide input, but provides output in the form of \texttt{csv} files, whose content can be matched to the program source, if a subsequent analysis has the appropriate internal representation. On the other hand, Soot is a compiler framework. Thus, using the Soot analysis results in a subsequent compilation phase is quite easy. Doop works quite well for producing analysis results, and not quite as well for using these results in a compilation process. Our Soot analysis also doesn't need to process the whole program for itself to produce the analysis results that we're interested in here---our intraprocedural analysis can use the existing in-memory representation and pass it on to the next phase, while Doop reads the whole program, throws it away, and leaves nothing for the next compilation phase. 

%% \paragraph{Expressiveness vs concision} In~\cite{bravenboer09:_stric_declar_specif_sophis_point_analy}, Bravenboer and Smaragdakis point out that:
%% \begin{quote}
%% Even conceptually clean program analysis algorithms that
%% rely on mutually recursive definitions often get transformed
%% into complex imperative code for implementation purposes.
%% \end{quote}
%% The presentation of the declarative approach in Section~\ref{sec:technique} could meaningfully include direct excerpts from the Datalog; including Java code is rarely meaningful, as there is too much boilerplate in that language.

%% The declarative approach takes 237 non-comment lines, compared to about 533 non-comment lines for the main part of the imperative approach, which is a significant point in favour of Doop. A head-to-head comparison is tricky, as the imperative approach also uses pre-analyses which are not present in the declarative approach.

%% We comment on the reasons for using helper analyses in the imperative version and not the declarative version. Recall that the helper analyses pre-computed information about 1) mock annotations and 2) constructors and setup methods. The mock annotations are an inessential difference; they could be computed on the fly in the imperative version, as they are in the declarative version. As for the constructors: when thinking imperatively, it is more intuitive to explicitly order the computations for constructors before regular test methods. On the other hand, thinking declaratively, it is more natural to use mutual recursion to declare a dependency on the results of previous computations for fields (our relation \texttt{isCollectionFieldThatContainsMocks} in particular) than to declare an explicit ordering. There is a small semantic difference in the two implementations, as the declarative implementation does not require field writes to be confined to constructors and setup methods; in this particular case, we empirically verified that the imperative assumption was almost always satisfied.

%% We also contrast how we store the abstraction in the two versions. The imperative version uses a standard dataflow analysis abstraction (three bits per local variable/field reference), along with an explicitly specified merge operator, while the declarative version uses one relation for each of the three bits. Propagating and merging data happens automatically in Doop.
%% % There is something going on with doop and kills, but I don't know enough of what's happening to meaningfully comment on it. Intersection-based analyses seem to be possible, because there is e.g. IntraproceduralMustPointTo. There's also something to do with phi nodes and redefinitions, but I can't clearly express it.

%% Another difference between the declarative and imperative versions is in the support for interprocedural analysis. As stated earlier, in Section~\ref{sec:technique}, the declarative version implements a context-insensitive interprocedural analysis while the imperative version is intraprocedural. The choice of intraprocedural versus interprocedural depends strongly on the particular analysis being implemented. Implementing the interprocedural analysis declaratively was impressively easy, while it is significantly more challenging to implement an interprocedural analysis in Soot, requiring the use of Heros~\cite{soap12ifds}, an additional framework. On the other hand, the Heros implementation would be IFDS-based and be context-sensitive; it would be somewhat harder to upgrade our context-insensitive implementat to a context-sensitive Doop implementation.

%% It is easier to add instrumentation, e.g. timers, to the imperative version than the declarative version. Doop contains some built-in timers, but it is unclear how to add new ones.

%% \paragraph{Development velocity}
%% To help the reader calibrate our descriptions, we describe our experience levels with Soot and Doop. One of the two authors has extensive experience with the Soot framework, while the other author started with no experience using Soot. Neither author was familiar with Doop at the beginning of this project. The Soot implementation was developed by the author who was unfamiliar with Soot, while the Doop implementation was developed by the other author. 

%% Soot is a mature program analysis framework and many of the common sticking points have, over the years, been addressed by the developers. Nevertheless, it can be intimidating to start working with Soot. Our experience with Doop is that it is overall robust, yet still being actively developed (i.e. occasionally, at the start, some daily snapshots didn't work with some versions of the underlying Souffl√© engine). There is more documentation for Soot than for Doop, although even for Doop, it is often possible to scrape together answers to one's questions from the source code and the online documentation. Finding the right API (or relation, in Doop) to use can be challenging for both Soot and Doop; it's impossible for us to fairly compare them, due to our different experiences with Soot and Doop.

%% Most of the time, adding a feature to the declarative version (e.g. field support) required an evening of work. This typically happened first; the declarative version is better for cleanly describing some approximation of the desired behaviour. Somewhat to our surprise, it was then possible to fast-follow with the imperative version, which ended up not taking much more than an evening to implement either. We believe that the existence of the declarative specification helped with designing the imperative version.

%% The declarative version was still subject to the combinatorial feature interaction problem; for instance, when we added support for fields and containers, we also specifically needed to add support for containers stored in fields.

%% Debugging is an inevitable part of any development process, including this one; declarative languages are no proof against debugging. Some Doop errors were just frustrating, e.g. hardcoding a syntactically incorrect method signature for a collection method. Other times, better type system enforcement in Datalog, and in particular, identifying relations that are unsatisfiable due to type conflicts, would help. Soot errors are typical programming errors.

%% Iteration speed can help with more effective debugging. On some benchmarks, Soot iterations could finish in under a minute, while Doop analysis-only iterations could finish in 10 seconds (but we didn't know that at the time). To expand on that: while developing our analysis, we ran our analysis together with the main analysis, and recomputed the main analysis every time we iterated. Yet, Doop supports running add-on analyses like ours, in isolation, after the main analysis terminates. If we were doing it again, we would develop our analysis as a run-after analysis. Running with the main analysis requires at least a 2.5-minute iteration time due to the necessity of re-compiling and re-running the entire analysis every time the analysis changes, while running an analysis after the main analysis can take 10 seconds, as mentioned above. Setting up the analysis to run after the main analysis is trickier and requires understanding of Doop which we did not have until late in the process. 

%% As stated above, instrumentation is easier in Soot than in Doop, and that extends to printing debug information and using traditional debugging tools, which works as well for Soot as traditional debugging does in general. To debug the Doop analysis, we resorted to outputting relevant relations after a Doop run and manually pinpointing which facts were missing or extraneous. Because Doop uses Soot to generate program facts, understanding Soot in particular and compilers in general was invaluable while developing the Doop implementation---we also looked at the Soot intermediate representation to understand what analysis information was flowing to which intermediate variables.

%% \paragraph{Summary} 
%We would conclude that, especially with the knowledge we have now gained about Doop, prototyping in Doop is easier than in Soot, but that it is no panacea; it remains subject to the feature interaction problem as well as debugging. Additionally, trying to add certain functional behaviours to the Doop implementation, such as timers, can be challenging.

%Finally, we have discussed our experience implementing this analysis twice, and pointed out the benefits and disadvantages of the imperative and declarative approaches for writing static analyses.



% cut here
%% Which base pointer analysis to use in the Doop? Assuming that we're talking about conservative call graphs (over-approximation), you would expect better call graphs to return fewer mocks. We performed experiments with the context-insensitive call graph versus the basic call graph and found XXX.

%% What we'll have for analysis results: context-insensitive intraproc; context-insensitive interproc; basic-only intraproc; basic-only interproc. The difference will be that the virtual method calls in the tests will be better resolved.

%% What we'll have for analysis timings: basic-only without mocks, basic-only with mocks, context-insensitive with mocks


%% \textsc{MockDetector} has demonstrated its capability of correctly identifying and tracing mock objects and containers containing them intra-procedurally in the test cases, in a suite of three benchmarks. It has the potential to be a helper static analysis tool, passing the mock information into existing static analysis frameworks for better call graph analysis. Future work of this project including adding the interprocedual analysis, and gathering results for more benchmarks. 